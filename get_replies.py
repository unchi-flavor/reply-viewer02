import json
import os
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from dateutil import parser
from dotenv import load_dotenv
from playwright.sync_api import sync_playwright

load_dotenv()

TARGET_USERS = os.getenv("TARGET_USERS", "elonmusk").split(",")
MAX_REPLIES = int(os.getenv("MAX_REPLIES", "100"))
DAYS_LIMIT = int(os.getenv("DAYS_LIMIT", "7"))

def get_mentions_for_user(page, username):
    replies = []
    search_url = f"https://twitter.com/search?q=to%3A{username}&f=live"
    print(f"[INFO] ã‚¢ã‚¯ã‚»ã‚¹URL: {search_url}")
    page.goto(search_url)
    time.sleep(5)

    # ãƒ‡ãƒãƒƒã‚°: ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ä¿å­˜ã—ã¦ã¿ã‚‹
    with open(f"debug_{username}.html", "w", encoding="utf-8") as f:
        f.write(page.content())
    print(f"[INFO] debug_{username}.html ã«HTMLã‚’ä¿å­˜ã—ã¾ã—ãŸ")

    # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã§ãã‚‹ã ã‘å¤šãå–å¾—ã™ã‚‹
    for i in range(3):
        page.keyboard.press("PageDown")
        time.sleep(1)
        print(f"[DEBUG] PageDown {i+1}å›ç›®")

    soup = BeautifulSoup(page.content(), "html.parser")
    articles = soup.find_all("article")
    print(f"[INFO] è¦‹ã¤ã‹ã£ãŸarticleã‚¿ã‚°æ•°: {len(articles)}")

    for idx, article in enumerate(articles):
        text_elem = article.find("div", attrs={"data-testid": "tweetText"})
        if not text_elem:
            continue
        text = text_elem.text.strip()
        print(f"[DEBUG] {idx+1}ä»¶ç›®ãƒªãƒ—æœ¬æ–‡: {text[:60]}...")

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åï¼ˆè¡¨ç¤ºåã¨IDï¼‰å–å¾—
        user_block = article.find("div", attrs={"data-testid": "User-Name"})
        if user_block:
            username_reply = user_block.get_text(strip=True)
        else:
            username_reply = "unknown"
        print(f"[DEBUG] {idx+1}ä»¶ç›®ãƒªãƒ—ãƒ¦ãƒ¼ã‚¶ãƒ¼: {username_reply}")

        # ãƒ„ã‚¤ãƒ¼ãƒˆãƒªãƒ³ã‚¯
        tweet_link = article.find("a", href=True)
        tweet_url = "https://twitter.com" + tweet_link["href"] if tweet_link else ""
        print(f"[DEBUG] {idx+1}ä»¶ç›®ãƒ„ã‚¤ãƒ¼ãƒˆURL: {tweet_url}")

        # æŠ•ç¨¿æ™‚åˆ»ã‚’å–å¾—
        time_elem = article.find("time")
        if time_elem and time_elem.has_attr("datetime"):
            tweet_time = time_elem["datetime"]
        else:
            tweet_time = datetime.now().isoformat()
        print(f"[DEBUG] {idx+1}ä»¶ç›®æŠ•ç¨¿æ™‚åˆ»: {tweet_time}")

        now = datetime.now().isoformat()
        replies.append({
            "username": username_reply,
            "text": text,
            "timestamp": tweet_time,
            "reply_url": tweet_url,
            "reply_to_id": "",
            "collected_at": now,
            "original_text": f"@{username} å®›ã¦ã®æŠ•ç¨¿"
        })

        if len(replies) >= MAX_REPLIES:
            print(f"[INFO] MAX_REPLIES({MAX_REPLIES})ã«åˆ°é”ã—ãŸã®ã§break")
            break

    print(f"[INFO] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œ{username}ã€ã«ã¤ã„ã¦ãƒªãƒ—åé›†å®Œäº†: {len(replies)}ä»¶")
    return replies

def _within_range(timestamp_str, cutoff_dt):
    try:
        parsed = parser.parse(timestamp_str)
        result = parsed >= cutoff_dt
        print(f"[DEBUG] ãƒ•ã‚£ãƒ«ã‚¿åˆ¤å®š: {timestamp_str} >= {cutoff_dt} â†’ {result}")
        return result
    except Exception as e:
        print(f"[ERROR] æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {timestamp_str} ({e})")
        return False

def save_replies(new_replies):
    try:
        with open('replies.json', 'r', encoding='utf-8') as f:
            existing = json.load(f)
    except Exception:
        existing = []
        print("[WARN] replies.jsonãŒå­˜åœ¨ã—ãªã„ã‹èª­ã¿è¾¼ã¿å¤±æ•—ã€ç©ºãƒªã‚¹ãƒˆã§ç¶™ç¶š")

    existing_keys = {r['text'][:100] for r in existing}
    uniques = [r for r in new_replies if r['text'][:100] not in existing_keys]

    combined = sorted(existing + uniques, key=lambda x: x['collected_at'], reverse=True)[:1000]
    with open('replies.json', 'w', encoding='utf-8') as f:
        json.dump(combined, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ Saved {len(uniques)} new replies. Total: {len(combined)}")

def main():
    print(f"â–¶ï¸ Start: {datetime.now().isoformat()}")
    all_replies = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        for username in TARGET_USERS:
            print(f"[INFO] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œ{username.strip()}ã€ã®ãƒªãƒ—åé›†é–‹å§‹")
            replies = get_mentions_for_user(page, username.strip())
            all_replies.extend(replies)
        browser.close()

    cutoff = datetime.now() - timedelta(days=DAYS_LIMIT)
    print(f"[INFO] {DAYS_LIMIT}æ—¥å‰ã® {cutoff} ä»¥é™ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿")
    filtered = [r for r in all_replies if _within_range(r['timestamp'], cutoff)]
    print(f"[INFO] ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ä»¶æ•°: {len(filtered)}")
    save_replies(filtered)
    print("âœ… Done.")

if __name__ == "__main__":
    main()
